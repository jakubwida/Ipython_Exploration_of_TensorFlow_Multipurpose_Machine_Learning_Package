{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.123968\n",
      "y(x): -0.227288\n"
     ]
    }
   ],
   "source": [
    "\"\"\"simple optimizer: finding minimum of y=x+2x^2\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a =tf.constant(1.0,name=\"a\")\n",
    "b=tf.constant(2.0,name=\"b\")\n",
    "\n",
    "x=tf.Variable(1.0,name=\"x\")\n",
    "\n",
    "out = tf.mul(a,x)+tf.mul(b,tf.mul(x,x))\n",
    "\n",
    "out_summary=tf.scalar_summary(\"output_summary_optimization\", out)\n",
    "merged=tf.merge_all_summaries()\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(out)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.train.SummaryWriter('/tmp/tsb_log',sess.graph)\n",
    "\n",
    "    for i in range(1000):\n",
    "        sess.run(train_op)\n",
    "        summary=sess.run(merged)\n",
    "        writer.add_summary(summary,i)\n",
    "        \n",
    "    print(\"x:\",sess.run(out))# x where is min\n",
    "    print(\"y(x):\",sess.run(x))# min y\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic monolayer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Biases1,Biases2):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Biases_1,Biases_2)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "cost_summary=tf.scalar_summary(\"cost_summary_monolayer\", cost)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.train.SummaryWriter('/tmp/tsb_log',sess.graph)\n",
    "    for epoch in range(epoch_num):\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "            \n",
    "        summary = sess.run(cost_summary,feed_dict={Data: batch_x, Labels: batch_y})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy, test: 0.4201\n",
      "Accuracy, test: 0.5648\n",
      "Accuracy, test: 0.6039\n",
      "Accuracy, test: 0.6188\n",
      "Accuracy, test: 0.6365\n",
      "Accuracy, test: 0.663\n",
      "Accuracy, test: 0.6875\n",
      "Accuracy, test: 0.7097\n",
      "Accuracy, test: 0.7233\n",
      "Accuracy, test: 0.7316\n",
      "Accuracy, test: 0.7418\n",
      "Accuracy, test: 0.7577\n",
      "Accuracy, test: 0.775\n",
      "Accuracy, test: 0.7888\n",
      "Accuracy, test: 0.7981\n",
      "Accuracy: 0.7981\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic monolayer eval through test/train, not cost function\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Biases1,Biases2):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Biases_1,Biases_2)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "accuracy_train_summary=tf.scalar_summary(\"accuracy_train_summary_monolayer\", accuracy)\n",
    "accuracy_test_summary=tf.scalar_summary(\"accuracy_test_summary_monolayer\", accuracy)\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.train.SummaryWriter('/tmp/tsb_log',sess.graph)\n",
    "    for epoch in range(epoch_num):\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "        print (\"Accuracy, test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n",
    "        summary_test = sess.run(accuracy_test_summary,feed_dict={Data: mnist.test.images, Labels:  mnist.test.labels})\n",
    "        writer.add_summary(summary_test, epoch)\n",
    "        summary_train = sess.run(accuracy_train_summary,feed_dict={Data: mnist.train.images, Labels:  mnist.train.labels})\n",
    "        writer.add_summary(summary_train, epoch)        \n",
    "    \n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy-test: 0.9774\n",
      "Accuracy-train: 0.995818\n",
      "Accuracy-eval: 0.98\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer -with dropout & Adam Optimizer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu6(layer_1)#possible instead: sigmoid (80% acc), relu (90-96 % accuracy) \n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu6(layer_2)\n",
    "    \n",
    "    drop = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(drop,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "cost_summary=tf.scalar_summary(\"cost_summary_expanded\", cost)\n",
    "\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.train.SummaryWriter('/tmp/tsb_log',sess.graph)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y,keep_prob: 1.0 })\n",
    "            \n",
    "        summary = sess.run(cost_summary,feed_dict={Data: batch_x, Labels: batch_y,keep_prob: 1.0 })\n",
    "        writer.add_summary(summary, epoch)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Accuracy-test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-train:\", accuracy.eval({Data: mnist.train.images, Labels: mnist.train.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-eval:\", accuracy.eval({Data: mnist.validation.images, Labels: mnist.validation.labels,keep_prob: 1.0}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy, test: 0.9394\n",
      "Accuracy, test: 0.9659\n",
      "Accuracy, test: 0.9707\n",
      "Accuracy, test: 0.9736\n",
      "Accuracy, test: 0.9759\n",
      "Accuracy, test: 0.9761\n",
      "Accuracy, test: 0.9781\n",
      "Accuracy, test: 0.9799\n",
      "Accuracy, test: 0.9788\n",
      "Accuracy, test: 0.9776\n",
      "Accuracy, test: 0.9794\n",
      "Accuracy, test: 0.9782\n",
      "Accuracy, test: 0.9788\n",
      "Accuracy, test: 0.977\n",
      "Accuracy, test: 0.9807\n",
      "Accuracy-test: 0.9807\n",
      "Accuracy-train: 0.998564\n",
      "Accuracy-eval: 0.9824\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer -with dropout & Adam Optimizer- tensorboard accuracy evaluation\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu6(layer_1)#possible instead: sigmoid (80% acc), relu (90-96 % accuracy) \n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu6(layer_2)\n",
    "    \n",
    "    drop = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(drop,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "accuracy_train_summary=tf.scalar_summary(\"accuracy_train_summary_multilayer\", accuracy)\n",
    "accuracy_test_summary=tf.scalar_summary(\"accuracy_test_summary_mulitlayer\", accuracy)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.train.SummaryWriter('/tmp/tsb_log',sess.graph)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y,keep_prob: 1.0 })\n",
    "            \n",
    "        print (\"Accuracy, test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels,keep_prob: 1.0}))\n",
    "        summary_test = sess.run(accuracy_test_summary,feed_dict={Data: mnist.test.images, Labels:  mnist.test.labels,keep_prob: 1.0})\n",
    "        writer.add_summary(summary_test, epoch)\n",
    "        summary_train = sess.run(accuracy_train_summary,feed_dict={Data: mnist.train.images, Labels:  mnist.train.labels,keep_prob: 1.0})\n",
    "        writer.add_summary(summary_train, epoch)\n",
    "    \n",
    "  \n",
    "\n",
    "    print (\"Accuracy-test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-train:\", accuracy.eval({Data: mnist.train.images, Labels: mnist.train.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-eval:\", accuracy.eval({Data: mnist.validation.images, Labels: mnist.validation.labels,keep_prob: 1.0}))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
