{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "\"\"\"complete basics\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant(40,name=\"x\")\n",
    "y=tf.constant(50,name=\"y\")\n",
    "z = tf.Variable(x+y)\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init);\n",
    "print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello world'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"hello world\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "hello=tf.constant(\"hello world\",name=\"hello\")\n",
    "\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init);\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "\"\"\"complete basics- better method of running session\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant(40,name=\"x\")\n",
    "y=tf.constant(50,name=\"y\")\n",
    "z = tf.Variable(x+y)\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #all the stuff you need to do should be here\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 26 27]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"slight expansion - arrays\"\"\"\n",
    "#it appears thet there is no difference between methods of running the session\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant([15,16,17],name=\"x\")\n",
    "y=tf.constant(10,name=\"y\")\n",
    "z = tf.Variable(x+y)\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(init);\n",
    "print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"basic saver example, requires kernel reset after saving, before loading\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "number = tf.Variable(1.0,name=\"number\")\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"basic restoring example\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "number = tf.Variable(5.0,name=\"number\")\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(number))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(sess.run(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.123968\n",
      "y(x): -0.227288\n"
     ]
    }
   ],
   "source": [
    "\"\"\"simple optimizer: finding minimum of y=x+2x^2\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a =tf.constant(1.0,name=\"a\")\n",
    "b=tf.constant(2.0,name=\"b\")\n",
    "\n",
    "x=tf.Variable(1.0,name=\"x\")\n",
    "\n",
    "out = tf.mul(a,x)+tf.mul(b,tf.mul(x,x))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(out)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        sess.run(train_op)\n",
    "        \n",
    "    print(\"x:\",sess.run(out))\n",
    "    print(\"y(x):\",sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.248597\n",
      "-0.227288\n",
      "0.263627\n"
     ]
    }
   ],
   "source": [
    "\"\"\"simple 2d optimizer: finding minimum of y=x+2x^2-z+2z^2, with use of arrays\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a =tf.constant(1.0,name=\"a\")\n",
    "b=tf.constant(2.0,name=\"b\")\n",
    "c=tf.constant(-1.0,name=\"c\")\n",
    "d=tf.constant(2.0,name=\"d\")\n",
    "\n",
    "vars=tf.Variable([1.0,1.0],name=\"vars\") #vars contains x and z\n",
    "\n",
    "out = tf.mul(a,vars[0])+tf.mul(b,tf.mul(vars[0],vars[0]))+tf.mul(c,vars[1])+tf.mul(d,tf.mul(vars[1],vars[1]))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(out)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        sess.run(train_op)\n",
    "        \n",
    "    print(sess.run(out)) # min y\n",
    "    print(sess.run(vars[0]))# x\n",
    "    print(sess.run(vars[1]))# z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.335843\n",
      "0.199692\n"
     ]
    }
   ],
   "source": [
    "\"\"\"linear regression, partially from examples\"\"\"\n",
    "\n",
    "\n",
    "#\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random\n",
    "\n",
    "#\n",
    "# Training Data\n",
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "\n",
    "n_samples = train_X.shape[0]\n",
    "\n",
    "#\n",
    "# numbers\n",
    "epoch_num=1000;\n",
    "trainig_rate=0.001;\n",
    "\n",
    "# \n",
    "# model\n",
    "X=tf.placeholder(\"float\")\n",
    "Y=tf.placeholder(\"float\")\n",
    "\n",
    "W=tf.Variable(rng.randn(),name=\"weight\")\n",
    "b=tf.Variable(rng.randn(),name=\"bias\")\n",
    "\n",
    "pred = tf.add(tf.mul(X, W), b)\n",
    "\n",
    "#\n",
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(trainig_rate).minimize(cost)\n",
    "\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch_num):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "        \n",
    "    print(sess.run(W))\n",
    "    print(sess.run(b))\n",
    "    \n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy: 0.7954\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic network\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Biases1,Biases2):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Biases_1,Biases_2)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.05\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy: 0.9654\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer -with non relu activation function\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=15\n",
    "training_rate=0.05\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu6(layer_1)#possible instead: sigmoid (80% acc), relu (90-96 % accuracy) \n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu6(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "        \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy-test: 0.977\n",
      "Accuracy-train: 0.995073\n",
      "Accuracy-eval: 0.9764\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer -with dropout & Adam Optimizer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=10\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "\n",
    "\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu6(layer_1)#possible instead: sigmoid (80% acc), relu (90-96 % accuracy) \n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu6(layer_2)\n",
    "    \n",
    "    drop = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(drop,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y,keep_prob: 1.0 })\n",
    "        \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Accuracy-test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-train:\", accuracy.eval({Data: mnist.train.images, Labels: mnist.train.labels,keep_prob: 1.0}))\n",
    "    print (\"Accuracy-eval:\", accuracy.eval({Data: mnist.validation.images, Labels: mnist.validation.labels,keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy-test: 0.9783\n",
      "Accuracy-train: 0.995982\n",
      "Accuracy-eval: 0.9812\n"
     ]
    }
   ],
   "source": [
    "\"\"\"completely basic multilayer- overfiting research -Adam Optimizer, no dropout, eval on mnist.test/mnist.train/mnist.eval\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=10\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu6(layer_1)#possible instead: sigmoid (80% acc), relu (90-96 % accuracy) \n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu6(layer_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y })\n",
    "        \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy-test:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n",
    "    print (\"Accuracy-train:\", accuracy.eval({Data: mnist.train.images, Labels: mnist.train.labels}))\n",
    "    print (\"Accuracy-eval:\", accuracy.eval({Data: mnist.validation.images, Labels: mnist.validation.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"tensorflow official example- for comparison (mnist for beginners)\"\"\"\n",
    "\n",
    "\n",
    "global graph_xes\n",
    "global graph_ys\n",
    "\n",
    "graph_xes=[]\n",
    "graph_ys=[]\n",
    "\n",
    "def print_results():\n",
    "    global graph_xes\n",
    "    global graph_ys\n",
    "    %matplotlib inline\n",
    "    plt.plot(graph_xes,graph_ys)\n",
    "    plt.show()\n",
    "    graph_xes=[]\n",
    "    graph_ys=[]\n",
    "\n",
    "\n",
    "def doStuff(in_range, batch_size):\n",
    "    global graph_xes\n",
    "    global graph_ys\n",
    "    #different layer setup\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    #different annotation of cost function\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(in_range):\n",
    "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "      sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    print(\"in range \",in_range,\"batch size :\",batch_size)\n",
    "    result = sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})\n",
    "    graph_xes.append(batch_size)\n",
    "    graph_ys.append(result)\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "  \n",
    "for x in range(1,30):\n",
    "    doStuff(1000,x*10)\n",
    "\n",
    "print_results()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy: 0.9764\n",
      "Model saved in file: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"saver example -network to be saved\"\"\"\n",
    "#requires kernel resart to be later loaded\n",
    "#utilizes below network:\n",
    "\"\"\"completely basic multilayer -with Adam Optimizer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=30\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(training_rate).minimize(cost)\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "#complete network saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run([train_op, cost], feed_dict={Data: batch_x, Labels: batch_y})\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Model restored.\n",
      "Accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "\"\"\"loader example -network to be loaded\"\"\"\n",
    "#requires kernel resart before loading, uses above example to save\n",
    "#utilizes below network:\n",
    "\"\"\"completely basic multilayer -with Adam Optimizer\"\"\"\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#numbers:\n",
    "epoch_num=30\n",
    "training_rate=0.001\n",
    "batch_size=100\n",
    "\n",
    "#placeholders & variables\n",
    "Data=tf.placeholder(\"float32\",[None,784])\n",
    "Labels=tf.placeholder(\"float32\",[None,10])\n",
    "\n",
    "Weights_1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "Weights_2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "Weights_3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "\n",
    "Biases_1=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_2=tf.Variable(tf.random_normal([256],stddev=0.01))\n",
    "Biases_3=tf.Variable(tf.random_normal([10],stddev=0.01))\n",
    "\n",
    "#model\n",
    "def network(x,Weights1,Weights2,Weights3,Biases1,Biases2,Biases3):\n",
    "    layer_1=tf.add(tf.matmul(x,Weights1),Biases1)\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    " \n",
    "    layer_2=tf.add(tf.matmul(layer_1,Weights2),Biases2)\n",
    "    layer_2=tf.nn.relu(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,Weights3),Biases3)\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "pred =network(Data,Weights_1,Weights_2,Weights_3,Biases_1,Biases_2,Biases_3)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, Labels))\n",
    "\n",
    "\n",
    "\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "#complete network saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #model doesnt have any training code, yet it works after loading parameters\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Labels, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({Data: mnist.test.images, Labels: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
